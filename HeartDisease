#importing necessary libraries and scaling
import csv
import sys
import os
import pandas
import sklearn
import seaborn
import matplotlib
import numpy
from itertools import permutations, combinations
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np
import pandas as pd
from matplotlib import rcParams
from matplotlib.cm import rainbow
%matplotlib inline
# Machine Learning
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier

os.chdir(r"C:\Users\Sameer\Desktop\Major Project")
dataset=pandas.read_csv('heart.csv')
standardScaler = StandardScaler()
columns_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']
dataset[columns_to_scale] = standardScaler.fit_transform(dataset[columns_to_scale])
dataset=dataset.dropna(axis=0)
y = dataset['target']
X = dataset.drop(['target'],axis=1)
combination = list(X.columns)
comb = list(list(combinations(combination,6)))

#Support Vector Machine

def svm(X_train, X_test, y_train, y_test):
    svc_scores = []
    kernels = ['linear', 'poly', 'rbf', 'sigmoid']

    for i in range(len(kernels)):
        svc_classifier = SVC(kernel = kernels[i])
        svc_classifier.fit(X_train, y_train)
        svc_scores.append(round(svc_classifier.score(X_test, y_test),2))
   # colors = rainbow(numpy.linspace(0, 1, len(kernels)))
    #plt.bar(kernels, svc_scores, color = colors)
    #for i in range(len(kernels)):
     #   plt.text(i, svc_scores[i], svc_scores[i])
    #plt.xlabel('Kernels')
    #plt.ylabel('Scores')
    #plt.title('Support Vector Classifier scores for different kernels')
    #plt.show()
    #Using the linear SV

    svm_result = max(svc_scores)
    return(svm_result)
    
#K Nearest Neighbors

def knn(X_train, X_test, y_train, y_test):
    knn_scores = []
    for k in range(1,21):
        knn_classifier = KNeighborsClassifier(n_neighbors = k)
        knn_classifier.fit(X_train, y_train)
        knn=(knn_classifier.score(X_test, y_test))
        knn=round(knn,2)
        knn_scores.append(knn)
    #plt.figure(figsize=(20,18))
    #plt.plot([k for k in range(1, 21)], knn_scores, color = 'red')
    #for i in range(1,21):
     #   plt.text(i, knn_scores[i-1], (i, knn_scores[i-1]), fontsize=9 )
    #plt.xticks([i for i in range(1, 21)])
    #plt.xlabel('Number of Neighbors (K)')
    #plt.ylabel('Scores')
    #plt.title('K Neighbors Classifier scores for different K values')
    #plt.show()
#Choosing max of all scores

    knn_result = max(knn_scores)
    return(knn_result)

#Decision Tree

def decisiontree(X_train, X_test, y_train, y_test):
    dt_scores = []

    for i in range(1, len(X_train.columns) + 1):
        dt_classifier = DecisionTreeClassifier(max_features = i, random_state = 0)
        dt_classifier.fit(X_train, y_train)
        dt_scores.append(round(dt_classifier.score(X_test, y_test),2))

    #plt.plot([i for i in range(1, len(X.columns) + 1)], dt_scores, color = 'green')
    #for i in range(1, len(X.columns) + 1):
     #   plt.text(i, dt_scores[i-1], (i, dt_scores[i-1]))
    #plt.xticks([i for i in range(1, len(X.columns) + 1)])
    #plt.xlabel('Max features')
    #plt.ylabel('Scores')
    #plt.title('Decision Tree Classifier scores for different number of maximum features')
    #plt.show()
    #Choosing tree with max accuracy 
    dt_result = max(dt_scores)
    return(dt_result)
    
#Random Forest

def randomforest(X_train, X_test, y_train, y_test):
    rf_scores = []
    estimators = [10, 100, 200, 500, 1000]
    for i in estimators:
        rf_classifier = RandomForestClassifier(n_estimators = i, random_state = 0)
        rf_classifier.fit(X_train, y_train)
        rf_scores.append(round(rf_classifier.score(X_test, y_test),2))

    #colors = rainbow(np.linspace(0, 1, len(estimators)))
    #plt.bar([i for i in range(len(estimators))], rf_scores, color = colors, width = 0.8)
   # for i in range(len(estimators)):
    #    plt.text(i, rf_scores[i], rf_scores[i])
   # ticks=[i for i in range(len(estimators))]
    #labels = [str(estimator) for estimator in estimators]
    #plt.xticks(ticks, labels)

   # plt.xlabel('Number of estimators')
    #plt.ylabel('Scores')
   # plt.title('Random Forest Classifier scores for different number of estimators')
    #plt.show()
    #Assigning maximum result
    rf_result = max(rf_scores)
    return(float(rf_result))
    
#Finding the Arithmetic mean of all the scores from the four differenct classification algorithms for all the combinations of features

def average(X_train, X_test, y_train, y_test):
    svm_score=svm(X_train, X_test, y_train, y_test)
    knn_score=knn(X_train, X_test, y_train, y_test)
    dt_score=decisiontree(X_train, X_test, y_train, y_test)
    rf_score=randomforest(X_train, X_test, y_train, y_test)
    #print(svm_score)
    #print(knn_score)
    #print(dt_score)
    #print(rf_score)
    score = float((svm_score+knn_score+dt_score+rf_score)/0.04)
    score=round(score,3)
    print(score)
    return score
#     scores.append(score)

# Since it is computationally heavy exercise with independent units, we can use multi-processes 
  
  
import random
import multiprocessing
from multiprocessing import Pool

import time
start = time.time()


def main_search(n_comb):
    # This function take one combination of features at a time and gives the columns + average score in a list 
    drop_fts= [f for f in dataset if f not in comb[n_comb]]
   # print(drop_fts)
    X = dataset.drop(drop_fts,axis=1)
    print(list(X.columns))
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.40, random_state = 2)
    score = average(X_train, X_test, y_train, y_test)
    features_scores_list = list(X.columns) + [score]
    return features_scores_list



if __name__ == "__main__":
# Main function with mulitple processors where we call the main search function. By using pool, 
# we will append the list generated into a single data frame
    print('in main.py')
    dataset=pandas.read_csv('heart.csv')
    standardScaler = StandardScaler()
    columns_to_scale = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']
    dataset[columns_to_scale] = standardScaler.fit_transform(dataset[columns_to_scale])
    dataset=dataset.dropna(axis=0)
    y = dataset['target']
    X = dataset.drop(['target'],axis=1)
    combination = list(X.columns)
    comb = list(list(combinations(combination,6)))
    
    
    jobs = []
    features_scores_list = []
    p = Pool(16)
    features_scores_list = p.map(main_search, list(range(0,1716)))
    features_scores = pd.DataFrame(columns = ['col1','col2','col3','col4','col5','col6','score']\
                                   ,data = features_scores_list)
    print(features_scores)
    print('The highest efficiency achieved is', np.max(features_scores['score']))
    
end = time.time()
print(end - start)
features_scores.to_csv('Combination_result.csv', index = False)
with open('Combination_result.csv', 'r') as file:
        reader = csv.DictReader(file)
        highest = sorted(reader, key=lambda x: (x['score']))[-100:]
        print(highest)
Top = pd.DataFrame(highest)
Top.to_csv('Top_Comb.csv',index = False)
with open('Top_Comb.csv', newline='') as f:
    next(f)
    reader = csv.reader(f)
    data = list(reader)
c1=0
c2=0
c3=0
c4=0
c5=0
c6=0
c7=0
c8=0
c9=0
c10=0
c11=0
c12=0
c13=0
for x in data[:-1]:
    if ('ca')  in x:
        #global c1
        c1=c1+1
    if ('trestbps')  in x:
        #global c2
        c2=c2+1
    if ('sex')  in x:
        #global c3
        c3=c3+1
    if ('fbs')  in x:
        #global c4
        c4=c4+1
    if ('thalach')  in x:
        #global c5
        c5=c5+1
    if ('exang')  in x:
        #global c6
        c6=c6+1
    if ('thal')  in x:
        #global c7
        c7=c7+1
    if ('chol')  in x:
        #global c8
        c8=c8+1
    if ('slope')  in x:
        #global c9
        c9=c9+1
    if ('oldpeak')  in x:
        #global c10
        c10=c10+1
    if ('restecg')  in x:
        #global c11
        c11=c11+1
    if ('age')  in x:
        #global c12
        c12=c12+1
    if ('cp')  in x:
        #global c13
        c13=c13+1
a = [c1,c2,c3,c4,c5,c6,c7,c8,c9,c10,c11,c12,c13]
b= ['ca','trestbps','sex','fbs','thalach','exang','thal','chol','slope','oldpeak','restecg','age','cp']
for i,j in zip(a,b):
    
    print(j, " occurs {} times".format(i))
